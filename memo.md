# つまったところ
- VPYLM の $p(c_1, c_2, .., c_k)$ における計算で、$p(c_1)$ や $p(c_2|c_1)$ ... が必要になる。単純に ${\rm addCutomer}(c_1|{\rm <BOW>})$ や ${\rm addCutomer}(c_2|{\rm <BOW>}, c_1)$ としていたが、これでは、短い文脈から生成されることが多くなる。結果としてすべての文字において、$p(c_i|\epsilon)$ から生成されることが多くなる。VPYLM の最大文脈長 $n-1$ とすると、$n-1$ 個の${\rm <BOS>}$ を文脈とした ${\rm addCutomer}(c_1|{\rm <BOW>}, ... {\rm <BOW>})$ をするべきでした。(n-gram 言語モデルを計算する人には常識かもしれない。)
  - VPYLMの計算は無限個の文脈長 $n-1$で重みづけするが実際には最大文脈長を設定する。最大文脈長以降にストップする確率をきちんと考慮して計算すると${\rm addCutomer}(c_1|{\rm <BOW>})$でも良くなった。この方が計算量を減らせる。
- $p(w, p_{os}) * \alpha[t][k][z]$ に比例する確率で単語 $w$ とその品詞 $p_{os}$ をサンプリングする際、内部では log で持っていたこれらの値を exp していたが、アンダーフローを起こしてしまう。logsumexp で計算するとよい。
    - logsumexp で計算するとよい。で計算してもアンダーフローする場合がある。

- PYHSMM において、 pos ごとに文字レベルの VPYLM を用意していたが、単語境界を固定して pos だけをサンプリングした場合の言語モデルが悪かった。pos ごとに別の VPYLM ではなく共通の VPYLM を使用したほうが良い。よく見ると、論文にも書いてあった。

|               Model                | perplexty |    n | number of POS tags |
| :--------------------------------: | --------: | ---: | -----------------: |
|               ngram                |     409.9 |    3 |                  - |
|               hpylm                |     130.4 |    3 |                  - |
|               vpylm                |     131.3 |    8 |                  - |
|               npylm                |     199.9 |    2 |                  - |
| pyhsmm (pos ごとに異なる文字VPYLM) |   24217.5 |      |                 10 |

- 初期値としてランダムな分割から始めたがそこまで自然な分割になってなかった。論文の通りにでは初期値1文を1単語してセットするとちゃんと動作した。

# 変更点
- $\alpha[t][k] = \sum_j p(c_{t-k-j+1}^{t-k}) * \alpha[t-k][j]$ から $\alpha[t][k] = \frac{1}{J} \sum_j p(c_{t-k-j+1}^{t-k}) * \alpha[t-k][j]$ とした。
  - 例えば、文字列 abcd があり、最大単語長 2, 文字語彙サイズを $V$ とすると、レストラン(パラメータ)が何もないときは、どの分割境界でもその言語モデルのスコア $LM = \sum_i p(w_i)$ は $\frac{1}{V^4}$ となる。しかし、もともとの前向き確率では、後ろ向きサンプリングすると、単語 d をサンプリングする確率が単語 cd をサンプリングする確率よりも明らかに高くなる。どの分割境界でも言語モデルのスコアが同じ値ならその分割境界も同程度にサンプリングされないといけないと考えため、変更した。(これが正しいかは分からない。)
- 単語長の補正にガンマ一般化線形回帰モデルは使用してない。
  - 未実装
- ひらがなやカタカナなどの文字種ごとの単語長補正など特別な配慮をしてない。日本語の分割では文字種ごとの補正は自然な分割のための有力な方法なので、論文中の例よりも不自然な分割結果な気がする。
- ポアソン分布による単語長の補正もしないほうが自然な分割に近いように見えるので、行わない。
  - 何度か実装を変更したので要確認

# メモ
- デバッグのためにエラーのキャッチを多数いれている。デバッグし終わったら、その部分をコメントアウトすることで高速化可能。C でいう#ifdef みたいな機能があればいいのだが。
- いまいち精度が出てないように見える。要調査。
- $p(c_1, c_2, .., c_k)$ における計算もアンダーフローを起こしているかもしれない。log で計算するべきだがしてない。

- gopy
    - 戻り値がnested slice に対応していない。(正確には、戻り値を受け取れるが要素
    にアクセスするとエラー)
    - 関数の引数に slice を受け取れない
    - 構造体の中にポインターがある場合、メモリ領域 がきちんと認識しているか不明
        - おそらく python側でポインタのポインタを認識しているか確認できるかが重要
        - pythonは勝手にgcが走ったりしないが、領域を確保している自覚がなければ、他のlinux プログラムが上書きしてしまう可能性がある？
        - 調査したところ、勝手に上書きすることはなさそう。(85%くらいの確信度)
            - 1 epoch 回したプログラムを停止させておき、その間に複数のプログラムでメモリがほぼいっぱいになるまで使う。その後、学習を再開させても不正なメモリアクセスしなかった。
    - pythonにおけるNPYLMクラスはHPYLMクラスを継承しているが、gopy によるbuildはNPYLMとHPYMのモジュールを書き込む順番が不定なので、import error になる場合がある。何度かbuildすると大丈夫になる。 