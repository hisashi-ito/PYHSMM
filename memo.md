# つまったところ
- VPYLM の $p(c_1, c_2, .., c_k)$ における計算で、$p(c_1)$ や $p(c_2|c_1)$ ... が必要になる。単純に ${\rm addCutomer}(c_1|{\rm <BOW>})$ や ${\rm addCutomer}(c_2|{\rm <BOW>}, c_1)$ としていたが、これでは、短い文脈から生成されることが多くなる。結果としてすべての文字において、$p(c_i|\epsilon)$ から生成されることが多くなる。VPYLM の最大文脈長 $n-1$ とすると、$n-1$ 個の${\rm <BOS>}$ を文脈とした ${\rm addCutomer}(c_1|{\rm <BOW>}, ... {\rm <BOW>})$ をするべきでした。(n-gram 言語モデルを計算する人には常識かもしれない。)
- $p(w, p_{os}) * \alpha[t][k][z]$ に比例する確率で単語 $w$ とその品詞 $p_{os}$ をサンプリングする際、内部では log で持っていたこれらの値を exp していたが、アンダーフローを起こしてしまう。logsumexp で計算するとよい。
    - logsumexp で計算するとよい。で計算してもアンダーフローする場合がある。

# 変更点
- $\alpha[t][k] = \sum_j p(c_{t-k-j+1}^{t-k}) * \alpha[t-k][j]$ から $\alpha[t][k] = \frac{1}{J} \sum_j p(c_{t-k-j+1}^{t-k}) * \alpha[t-k][j]$ とした。
  - 例えば、文字列 abcd があり、最大単語長 2, 文字語彙サイズを $V$ とすると、レストラン(パラメータ)が何もないときは、どの分割境界でもその言語モデルのスコア $LM = \sum_i p(w_i)$ は $\frac{1}{V^4}$ となる。しかし、もともとの前向き確率では、後ろ向きサンプリングすると、単語 d をサンプリングする確率が単語 cd をサンプリングする確率よりも明らかに高くなる。どの分割境界でも言語モデルのスコアが同じ値ならその分割境界も同程度にサンプリングされないといけないと考えため、変更した。(これが正しいかは分からない。)
- 単語長の補正にガンマ一般化線形回帰モデルは使用してない。
  - 未実装
- ひらがなやカタカナなどの文字種ごとの特別な配慮をしてない。

# メモ
- デバッグのためにエラーのキャッチを多数いれている。デバッグし終わったら、その部分をコメントアウトすることで高速化可能。C でいう#ifdef みたいな機能があればいいのだが。
- いまいち精度が出てないように見える。要調査。
- $p(c_1, c_2, .., c_k)$ における計算もアンダーフローを起こしているかもしれない。log で計算するべきだがしてない。